{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import random\n",
    "import shutil\n",
    "import librosa\n",
    "import scipy\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad = webrtcvad.Vad(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_hierarchy(path: str) -> dict:\n",
    "    \"\"\"Computes all paths in a recursive manner to a dictionary\"\"\"\n",
    "    hierarchy = {\n",
    "        'type': 'folder',\n",
    "        'name': os.path.basename(path),\n",
    "        'path': path,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        hierarchy['children'] = [\n",
    "            path_hierarchy(os.path.join(path, contents))\n",
    "            for contents in os.listdir(path)\n",
    "        ]\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.ENOTDIR:\n",
    "            raise\n",
    "        hierarchy['type'] = 'file'\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "\n",
    "def split_speaker_wav(level):\n",
    "    \"\"\"Splits path into speaker path and corresponding wav file\"\"\"\n",
    "    root = \"/\".join(level['path'].split(\"/\")[:-2]) + \"/\"\n",
    "    filename = \"/\".join(level['path'].split(\"/\")[-2:])\n",
    "    return root, filename\n",
    "\n",
    "def get_speaker_wav_dict(root):\n",
    "    \"\"\"Returns a dictionary with speakers paths as keys and corresponding wavs as values\"\"\"\n",
    "    data_paths = defaultdict(list)\n",
    "    for level_a in path_hierarchy(root)['children']:\n",
    "        for level_b in level_a['children']:\n",
    "            for level_c in level_b['children']:\n",
    "                if level_c['type'] == 'folder':\n",
    "                    for level_d in level_c['children']:\n",
    "                        root, filename = split_speaker_wav(level_d)\n",
    "                        data_paths[root].append(filename)\n",
    "                else:\n",
    "                    root, filename = split_speaker_wav(level_c)\n",
    "                    data_paths[root].append(filename)\n",
    "    return data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise_path(noise_folders):\n",
    "    \"\"\"Randomly samples a noise wav file\"\"\"\n",
    "    sample_folder = random.choice(noise_folders)\n",
    "    sample_files = os.listdir(sample_folder)\n",
    "    sample_filepath = sample_folder + random.choice(sample_files)\n",
    "    return sample_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failed_paths(train_paths):\n",
    "    \"\"\"Return files that cannot be used for training\"\"\"\n",
    "    failed = []\n",
    "    for train_path in tqdm(train_paths):\n",
    "        db_dir = train_path.replace(\"wav\", \"meldb\")\n",
    "        for wav_path in train_paths[train_path]:\n",
    "            npy_path = wav_path.replace('.wav', '.npy')\n",
    "            db_path = db_dir + npy_path\n",
    "            audio_path = train_path + wav_path\n",
    "            try:\n",
    "                mel_db = np.load(db_path)\n",
    "                if mel_db.shape[0] < 180:\n",
    "                    failed.append(wav_path)\n",
    "            except Exception as e:\n",
    "                failed.append(wav_path)\n",
    "    return failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speech processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend(signal, noise, target_snr=10):\n",
    "    \"\"\"Blends audio signal with noise with target SNR\"\"\"\n",
    "    if len(signal) < len(noise):\n",
    "        noise = noise[:len(signal)]\n",
    "    elif len(noise) < len(signal):\n",
    "        signal = signal[:len(noise)]\n",
    "\n",
    "    if target_snr == np.inf:\n",
    "        scaler, prescaler = 0, 1\n",
    "    elif target_snr == -np.inf:\n",
    "        scaler, prescaler = 1, 0\n",
    "    else:\n",
    "        signal_power = np.sum(signal**2)\n",
    "        noise_power = np.sum(noise**2)\n",
    "        scaler = np.sqrt( signal_power / (noise_power * 10.**(target_snr/10.)) )\n",
    "        prescaler = 1\n",
    "\n",
    "    return prescaler * signal + scaler * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech(x, sample_rate, frame_duration, hop_duration):\n",
    "    \"\"\"Removes silences from speech\"\"\"\n",
    "    frame_length = int(frame_duration * sample_rate)\n",
    "    hop_length = int(hop_duration * sample_rate)\n",
    "    frames = scipy.array([x[i:i+frame_length] for i in range(0, len(x) - frame_length, hop_length)])\n",
    "    is_speech = [vad.is_speech(frame.tobytes(), sample_rate) for frame in frames]\n",
    "    speech_indices = np.nonzero(is_speech)\n",
    "    speech_only = frames[speech_indices].flatten()\n",
    "    return speech_only\n",
    "\n",
    "def get_meldb(path, noise_folders, audio_params):\n",
    "    \"\"\"Returns 40 filter banks\"\"\"\n",
    "    x, _ = librosa.core.load(path, sr=audio_params['sr'])\n",
    "    if random.choice([True, True, False]):\n",
    "        noise_path = sample_noise_path(noise_folders)\n",
    "        noise, _ = librosa.core.load(noise_path, sr=audio_params['sr'])\n",
    "        mean_snr = random.choice([7.5, 10 , 12.5, 15, 17.5, 20])\n",
    "        std_snr = random.uniform(0.1, 1)\n",
    "        snr = np.random.normal(mean_snr, std_snr)\n",
    "        x = blend(x, noise, target_snr=snr)\n",
    "    x = get_speech(x, _, 0.01, 0.01)\n",
    "    window_length = int(audio_params['window'] * audio_params['sr'])\n",
    "    hop_length = int(audio_params['hop'] * audio_params['sr'])\n",
    "    spec = librosa.stft(x, n_fft=audio_params['nfft'],\n",
    "                        hop_length=hop_length, \n",
    "                        win_length=window_length)\n",
    "    mag_spec = np.abs(spec)\n",
    "    mel_basis = librosa.filters.mel(audio_params['sr'], audio_params['nfft'],\n",
    "                                    n_mels=audio_params['nmels'])\n",
    "    mel_spec = np.dot(mel_basis, mag_spec)\n",
    "    mel_db = librosa.amplitude_to_db(mel_spec).T\n",
    "    return mel_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = \"./dataset/dev/wav\"\n",
    "test_root = \"./dataset/test/wav\"\n",
    "noise_root = \"./dataset/QUT-NOISE/split_noises/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = get_speaker_wav_dict(train_root)\n",
    "test_paths = get_speaker_wav_dict(test_root)\n",
    "noise_folders = [noise_root + file + \"/\" for file in os.listdir(noise_root)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speakers in train:  1211\n",
      "Total speakers in test:  40\n",
      "Total files in train:  148642\n",
      "Total files in test:  4874\n"
     ]
    }
   ],
   "source": [
    "print(\"Total speakers in train: \", len(train_paths))\n",
    "print(\"Total speakers in test: \", len(test_paths))\n",
    "print(\"Total files in train: \", sum([len(train_paths[root]) for root in train_paths]))\n",
    "print(\"Total files in test: \", sum([len(test_paths[root]) for root in test_paths]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute features for training speedup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_params = {'mean_snr': 15,\n",
    "#                'sd_snr':10,\n",
    "#                'hop': 0.01,\n",
    "#                'window': 0.025,\n",
    "#                'sr': 16000,\n",
    "#                'nfft': 512,\n",
    "#                'nmels': 40\n",
    "#                }\n",
    "\n",
    "# failed = []\n",
    "# for train_path in tqdm(train_paths):\n",
    "#     db_dir = train_path.replace(\"wav\", \"meldb\")\n",
    "#     for wav_path in train_paths[train_path]:\n",
    "#         npy_path = wav_path.replace('.wav', '.npy')\n",
    "#         db_path = db_dir + npy_path\n",
    "#         audio_path = train_path + wav_path\n",
    "#         os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "#         db = get_meldb(audio_path, noise_folders, audio_params)\n",
    "#         if db.shape[0] >= 140:\n",
    "#             np.save(db_path, db)\n",
    "#         else:\n",
    "#             failed.append(wav_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxCelebDatasest(Dataset):\n",
    "    def __init__(self, train_paths, noise_folders, failed, M, N, training=True):\n",
    "        self.batch_size = M\n",
    "        self.failed = set(failed)\n",
    "        self.count = 0\n",
    "        self.segment_size = None\n",
    "        if training:\n",
    "            self.paths = train_paths\n",
    "            self.noise_folders = noise_folders\n",
    "            self.N = N\n",
    "        self.speakers = list(train_paths)\n",
    "        random.shuffle(self.speakers)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.speakers)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        speaker = self.speakers[idx]\n",
    "        wav_files = self.paths[speaker]\n",
    "        wav_files = [file for file in wav_files if file not in failed]\n",
    "        wav_files = random.sample(wav_files, self.N)\n",
    "        \n",
    "        self.keep_segment_size()\n",
    "        mel_dbs = []\n",
    "        for f in wav_files:\n",
    "            mel_db = np.load(speaker.replace(\"/wav/\", \"/meldb/\") + f.replace(\".wav\", \".npy\"))\n",
    "            self.segment_size = min(self.segment_size, len(mel_db))\n",
    "            mel_dbs.append(mel_db)\n",
    "            \n",
    "        for idx, mel_db in enumerate(mel_dbs):\n",
    "            last = len(mel_db) - self.segment_size\n",
    "            beg = random.randint(0, last)\n",
    "            mel_db = mel_db[beg:beg + self.segment_size]\n",
    "            mel_dbs[idx] = mel_db\n",
    "        \n",
    "        return torch.Tensor(mel_dbs)\n",
    "    \n",
    "    def keep_segment_size(self):\n",
    "        self.count += 1\n",
    "        if self.count % (self.batch_size + 1) == 0:\n",
    "            self.count = 1\n",
    "        if self.count == 1:\n",
    "            self.segment_size = random.randint(140, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GE2EModel(nn.Module):\n",
    "    def __init__(self, nlstm, dembed, dhid, dout, dropout=0.):\n",
    "        super(GE2EModel, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(dembed, dhid, num_layers=nlstm, batch_first=True)\n",
    "        self.linear = nn.Linear(dhid, dout)\n",
    "    def forward(self, inp):\n",
    "        lstm_out, _ = self.lstm(inp)\n",
    "        out = self.linear(lstm_out[:, -1, :])\n",
    "        out_norm = out / torch.norm(out, p=2, dim=0)\n",
    "        return out_norm\n",
    "            \n",
    "        \n",
    "class GE2ELoss(nn.Module):\n",
    "    def __init__(self, M):\n",
    "        super(GE2ELoss, self).__init__()\n",
    "        self.w = nn.Parameter(torch.tensor(10.0).to(device), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.tensor(-5.0).to(device), requires_grad=True)\n",
    "        self.M = M\n",
    "        self.device = device\n",
    "    def forward(self, k):\n",
    "        groups = torch.split(k, self.M, dim=0)\n",
    "        centroids = torch.stack([torch.mean(group, dim=0) for group in groups])\n",
    "        sims = []\n",
    "        for idx, item in enumerate(k.repeat(N, 1, 1).transpose(1, 0)):\n",
    "            cur_centroid_idx = idx // self.M\n",
    "            for_stability = (centroids[cur_centroid_idx] - item[0]) * self.M / (self.M - 1)\n",
    "            updated_centroids = centroids.clone()\n",
    "            updated_centroids[cur_centroid_idx] = for_stability\n",
    "            sims.append(F.cosine_similarity(item, updated_centroids, dim=-1, eps=1e-8))\n",
    "        cossim = torch.stack(sims)\n",
    "        sim_matrix = self.w * cossim + self.b\n",
    "        loss = self.compute_loss(sim_matrix)\n",
    "        return loss\n",
    "    def compute_loss(self, sim_matrix):\n",
    "        loss = 0 \n",
    "        for idx, group in enumerate(torch.split(sim_matrix, self.M, dim=0)):\n",
    "            for utterance_sims in group:\n",
    "                cur = utterance_sims[idx]\n",
    "                rest = torch.cat([utterance_sims[:idx], utterance_sims[idx+1:]])\n",
    "                loss += - cur + torch.log(torch.sum(torch.exp(rest)))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(PATH, epoch, ge2e_model, ge2e_loss, optimizer, loss):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'ge2e_model_state_dict': ge2e_model.state_dict(),\n",
    "            'ge2e_loss_state_dict': ge2e_loss.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, PATH)\n",
    "    print(\"SAVED weights! @ epoch {}\".format(epoch))\n",
    "    \n",
    "def load_model(PATH, ge2e_model, ge2e_loss, optimizer):\n",
    "    checkpoint = torch.load(PATH)\n",
    "    ge2e_model.load_state_dict(checkpoint['ge2e_model_state_dict'])\n",
    "    ge2e_loss.load_state_dict(checkpoint['ge2e_loss_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return ge2e_model, ge2e_loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, ge2e_model, ge2e_loss, optimizer, epoch, batch_size):\n",
    "    shutil.rmtree(\"runs\")\n",
    "    writer = SummaryWriter()\n",
    "    ge2e_model.train()\n",
    "    total_loss = 0\n",
    "    with SummaryWriter() as writer:\n",
    "        for batch_id, mel_db_batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            num_audio_per_batch = mel_db_batch.size(0) * mel_db_batch.size(1)\n",
    "            mel_db_batch = torch.reshape(mel_db_batch, \n",
    "                                         (num_audio_per_batch, \n",
    "                                          mel_db_batch.size(2), \n",
    "                                          mel_db_batch.size(3)))\n",
    "            \n",
    "            perm = random.sample(range(0, num_audio_per_batch), num_audio_per_batch)\n",
    "            unperm = list(perm)\n",
    "            for idx, org_idx in enumerate(perm):\n",
    "                unperm[org_idx] = idx\n",
    "            \n",
    "            mel_db_batch = mel_db_batch[perm]\n",
    "            embeddings = ge2e_model(mel_db_batch.cuda())\n",
    "            embeddings = embeddings[unperm]\n",
    "            loss = ge2e_loss(embeddings)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(ge2e_model.parameters(), 3.0)\n",
    "            torch.nn.utils.clip_grad_norm_(ge2e_loss.parameters(), 1.0)\n",
    "            \n",
    "            # Scale gradients\n",
    "            for name, param in ge2e_model.named_parameters(): \n",
    "                if 'linear' in name:\n",
    "                    param.grad *= 0.5\n",
    "            for name, param in ge2e_loss.named_parameters():\n",
    "                param.grad *= 0.01\n",
    "                \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / (batch_id + 1)\n",
    "            \n",
    "            if (batch_id + 1) % 5 == 0:\n",
    "                for name, param in ge2e_model.named_parameters():          \n",
    "                    writer.add_histogram('ge2e_model.' + name, param, batch_id + 1)\n",
    "                for name, param in ge2e_loss.named_parameters():          \n",
    "                    writer.add_histogram('ge2e_loss.' + name, param, batch_id + 1)\n",
    "                print(\"epoch {:3d}, batch {:3d}, loss {:3.2f}, mean_loss {:3.2f}\".format(epoch, batch_id + 1, loss.item(), avg_loss))\n",
    "            writer.add_scalar('epoch_{}/avg_loss'.format(epoch), avg_loss, batch_id + 1)\n",
    "            writer.add_scalar('epoch_{}/batch_loss'.format(epoch), loss.item(), batch_id + 1)\n",
    "    \n",
    "    print(\"epoch {}, loss {}\".format(epoch, avg_loss))\n",
    "    os.makedirs('weights', exist_ok=True)\n",
    "    PATH = \"weights/epoch_{}_param_dict\".format(epoch)\n",
    "    save_model(PATH, epoch, ge2e_model, ge2e_loss, optimizer, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, nepoch, batch_size, saved_epoch=None, device='cuda'):\n",
    "    ge2e_model = GE2EModel(nlstm=3, dembed=40, dhid=768, dout=256).to(device)\n",
    "    ge2e_loss = GE2ELoss(M).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD([\n",
    "            {'params': ge2e_model.parameters()},\n",
    "            {'params': ge2e_loss.parameters()}\n",
    "            ], lr= 0.01)\n",
    "    \n",
    "    if saved_epoch:\n",
    "        PATH = \"weights/epoch_{}_param_dict\".format(saved_epoch)\n",
    "        ge2e_model, ge2e_loss, optimizer = load_model(PATH, ge2e_model, ge2e_loss, optimizer)\n",
    "        beg_epoch = saved_epoch\n",
    "        \n",
    "    else:\n",
    "        beg_epoch = 0\n",
    "    \n",
    "    with SummaryWriter(comment='GE2EModel') as w:\n",
    "        w.add_graph(GE2EModel(nlstm=3, dembed=40, dhid=768, dout=256), torch.randn(20, 140, 40), False)\n",
    "    \n",
    "    for epoch in range(beg_epoch, beg_epoch + nepoch):\n",
    "        train_epoch(train_loader, ge2e_model, ge2e_loss, optimizer, epoch + 1, batch_size)\n",
    "        \n",
    "    return ge2e_model, ge2e_loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(test_loader, ge2e_model, ge2e_loss, epoch, device='cuda'):\n",
    "    ge2e_model.eval()\n",
    "    for batch_id, mel_db_batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params = {'mean_snr': 15,\n",
    "               'sd_snr':10,\n",
    "               'hop': 0.01,\n",
    "               'window': 0.025,\n",
    "               'sr': 16000,\n",
    "               'nfft': 512,\n",
    "               'nmels': 40\n",
    "               }\n",
    "M = 4 # Speakers\n",
    "N = 5 # Utterances\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1211/1211 [01:29<00:00, 13.48it/s]\n"
     ]
    }
   ],
   "source": [
    "failed = get_failed_paths(train_paths)\n",
    "train_dataset = VoxCelebDatasest(train_paths, noise_folders, failed, M, N)\n",
    "train_loader = DataLoader(train_dataset, batch_size=M, shuffle=True, \n",
    "                          num_workers=4, drop_last=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1, batch   5, loss 270.04, mean_loss 274.65\n",
      "epoch   1, batch  10, loss 256.41, mean_loss 269.01\n",
      "epoch   1, batch  15, loss 244.48, mean_loss 262.33\n",
      "epoch   1, batch  20, loss 236.50, mean_loss 256.45\n",
      "epoch   1, batch  25, loss 230.41, mean_loss 251.49\n",
      "epoch   1, batch  30, loss 222.50, mean_loss 247.42\n",
      "epoch   1, batch  35, loss 225.36, mean_loss 244.06\n",
      "epoch   1, batch  40, loss 208.40, mean_loss 240.89\n",
      "epoch   1, batch  45, loss 207.44, mean_loss 238.10\n",
      "epoch   1, batch  50, loss 213.26, mean_loss 235.65\n",
      "epoch   1, batch  55, loss 222.66, mean_loss 233.88\n",
      "epoch   1, batch  60, loss 225.68, mean_loss 232.62\n",
      "epoch   1, batch  65, loss 209.42, mean_loss 230.77\n",
      "epoch   1, batch  70, loss 196.86, mean_loss 229.17\n",
      "epoch   1, batch  75, loss 215.31, mean_loss 227.82\n",
      "epoch   1, batch  80, loss 218.56, mean_loss 226.74\n",
      "epoch   1, batch  85, loss 195.11, mean_loss 225.59\n",
      "epoch   1, batch  90, loss 180.02, mean_loss 223.89\n",
      "epoch   1, batch  95, loss 205.41, mean_loss 222.76\n",
      "epoch   1, batch 100, loss 193.45, mean_loss 221.52\n",
      "epoch   1, batch 105, loss 207.14, mean_loss 220.99\n",
      "epoch   1, batch 110, loss 200.21, mean_loss 219.96\n",
      "epoch   1, batch 115, loss 212.04, mean_loss 219.26\n",
      "epoch   1, batch 120, loss 210.85, mean_loss 218.63\n",
      "epoch   1, batch 125, loss 201.54, mean_loss 217.90\n",
      "epoch   1, batch 130, loss 224.68, mean_loss 217.54\n",
      "epoch   1, batch 135, loss 221.69, mean_loss 216.91\n",
      "epoch   1, batch 140, loss 214.73, mean_loss 216.55\n",
      "epoch   1, batch 145, loss 208.95, mean_loss 215.80\n",
      "epoch   1, batch 150, loss 197.04, mean_loss 215.34\n",
      "epoch   1, batch 155, loss 202.65, mean_loss 214.85\n",
      "epoch   1, batch 160, loss 198.55, mean_loss 214.29\n",
      "epoch   1, batch 165, loss 208.94, mean_loss 214.02\n",
      "epoch   1, batch 170, loss 187.18, mean_loss 213.26\n",
      "epoch   1, batch 175, loss 198.68, mean_loss 212.83\n",
      "epoch   1, batch 180, loss 215.59, mean_loss 212.26\n",
      "epoch   1, batch 185, loss 194.47, mean_loss 211.71\n",
      "epoch   1, batch 190, loss 232.23, mean_loss 211.63\n",
      "epoch   1, batch 195, loss 204.50, mean_loss 211.52\n",
      "epoch   1, batch 200, loss 211.56, mean_loss 211.25\n",
      "epoch   1, batch 205, loss 196.11, mean_loss 210.91\n",
      "epoch   1, batch 210, loss 198.15, mean_loss 210.51\n",
      "epoch   1, batch 215, loss 178.42, mean_loss 210.08\n",
      "epoch   1, batch 220, loss 221.04, mean_loss 210.00\n",
      "epoch   1, batch 225, loss 181.16, mean_loss 209.47\n",
      "epoch   1, batch 230, loss 206.35, mean_loss 209.01\n",
      "epoch   1, batch 235, loss 183.05, mean_loss 208.55\n",
      "epoch   1, batch 240, loss 194.00, mean_loss 208.40\n",
      "epoch   1, batch 245, loss 191.57, mean_loss 207.92\n",
      "epoch   1, batch 250, loss 228.65, mean_loss 207.83\n",
      "epoch   1, batch 255, loss 191.30, mean_loss 207.61\n",
      "epoch   1, batch 260, loss 200.69, mean_loss 207.54\n",
      "epoch   1, batch 265, loss 185.59, mean_loss 207.29\n",
      "epoch   1, batch 270, loss 192.61, mean_loss 207.07\n",
      "epoch   1, batch 275, loss 212.12, mean_loss 206.78\n",
      "epoch   1, batch 280, loss 165.33, mean_loss 206.35\n",
      "epoch   1, batch 285, loss 180.70, mean_loss 206.01\n",
      "epoch   1, batch 290, loss 204.07, mean_loss 205.83\n",
      "epoch   1, batch 295, loss 214.55, mean_loss 205.77\n",
      "epoch   1, batch 300, loss 164.86, mean_loss 205.33\n",
      "epoch 1, loss 205.20526193782985\n",
      "SAVED weights! @ epoch 1\n"
     ]
    }
   ],
   "source": [
    "nepoch, batch_size  = 1, M\n",
    "ge2e_model, ge2e_loss, optimizer = train(train_loader, nepoch, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing and noise generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise_paths = [\"./dataset/QUT-NOISE/\" + file for file in os.listdir(\"./dataset/QUT-NOISE/\") if file.endswith(\".wav\")]\n",
    "# for noise_path in tqdm(noise_paths):\n",
    "#     cur_noise_dir = noise_path.replace(\".wav\", \"\").replace(\"-1\", \"\").replace(\"-2\", \"\")\n",
    "#     cur_noise_dir_split = cur_noise_dir.split(\"/\")\n",
    "#     cur_noise_dir_split.insert(-1, \"split_noises\")\n",
    "#     cur_noise_dir = \"/\".join(cur_noise_dir_split) + \"/\"\n",
    "#     if not os.path.exists(cur_noise_dir):\n",
    "#         os.makedirs(cur_noise_dir)\n",
    "#     wpath = cur_noise_dir + noise_path.split(\"/\")[-1].replace(\".wav\", \"\")\n",
    "#     os.system(\"ffmpeg -i {} -f segment -segment_time 2 -c copy {}%05d.wav\".format(noise_path, wpath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment = random.randint(140, 180)\n",
    "# sample_speaker = random.sample(list(train_paths), 1)[0]\n",
    "# sample_file = random.sample(train_paths[sample_speaker], 1)[0]\n",
    "# path = sample_speaker + sample_file\n",
    "# meldb, mixed = get_meldb(path, noise_folders, audio_params, segment)\n",
    "# ipd.Audio(mixed, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
