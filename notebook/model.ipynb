{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import random\n",
    "import librosa\n",
    "import scipy\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad = webrtcvad.Vad(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_hierarchy(path):\n",
    "    hierarchy = {\n",
    "        'type': 'folder',\n",
    "        'name': os.path.basename(path),\n",
    "        'path': path,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        hierarchy['children'] = [\n",
    "            path_hierarchy(os.path.join(path, contents))\n",
    "            for contents in os.listdir(path)\n",
    "        ]\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.ENOTDIR:\n",
    "            raise\n",
    "        hierarchy['type'] = 'file'\n",
    "\n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(level):\n",
    "    root = \"/\".join(level['path'].split(\"/\")[:-2]) + \"/\"\n",
    "    filename = \"/\".join(level['path'].split(\"/\")[-2:])\n",
    "    return root, filename\n",
    "def get_path_jsons(root):\n",
    "    data_paths = defaultdict(list)\n",
    "    for level_a in path_hierarchy(root)['children']:\n",
    "        for level_b in level_a['children']:\n",
    "            for level_c in level_b['children']:\n",
    "                if level_c['type'] == 'folder':\n",
    "                    for level_d in level_c['children']:\n",
    "                        root, filename = get_paths(level_d)\n",
    "                        data_paths[root].append(filename)\n",
    "                else:\n",
    "                    root, filename = get_paths(level_c)\n",
    "                    data_paths[root].append(filename)\n",
    "    return data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend(signal, noise, target_snr=10):\n",
    "    if len(signal) < len(noise):\n",
    "        noise = noise[:len(signal)]\n",
    "    elif len(noise) < len(signal):\n",
    "        signal = signal[:len(noise)]\n",
    "\n",
    "    if target_snr == np.inf:\n",
    "        scaler, prescaler = 0, 1\n",
    "    elif target_snr == -np.inf:\n",
    "        scaler, prescaler = 1, 0\n",
    "    else:\n",
    "        signal_power = np.sum(signal**2)\n",
    "        noise_power = np.sum(noise**2)\n",
    "        scaler = np.sqrt( signal_power / (noise_power * 10.**(target_snr/10.)) )\n",
    "        prescaler = 1\n",
    "\n",
    "    # blend\n",
    "    return prescaler * signal + scaler * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise_path(noise_folders):\n",
    "    sample_folder = random.choice(noise_folders)\n",
    "    sample_files = os.listdir(sample_folder)\n",
    "    sample_filepath = sample_folder + random.choice(sample_files)\n",
    "    return sample_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech(x, sample_rate, frame_duration, hop_duration):\n",
    "    frame_length = int(frame_duration * sample_rate)\n",
    "    hop_length = int(hop_duration * sample_rate)\n",
    "    frames = scipy.array([x[i:i+frame_length] for i in range(0, len(x) - frame_length, hop_length)])\n",
    "    is_speech = [vad.is_speech(frame.tobytes(), sample_rate) for frame in frames]\n",
    "    speech_indices = np.nonzero(is_speech)\n",
    "    speech_only = frames[speech_indices].flatten()\n",
    "    return speech_only\n",
    "\n",
    "def get_meldb(path, noise_folders, audio_params, tisv_frame):\n",
    "    x, _ = librosa.core.load(path, sr=audio_params['sr'])\n",
    "    if random.choice([True, True, False]):\n",
    "        noise_path = sample_noise_path(noise_folders)\n",
    "        noise, _ = librosa.core.load(noise_path, sr=audio_params['sr'])\n",
    "        mean_snr = random.choice([7.5, 10 , 12.5, 15, 17.5, 20])\n",
    "        std_snr = random.uniform(0.1, 1)\n",
    "        snr = np.random.normal(mean_snr, std_snr)\n",
    "        x = blend(x, noise, target_snr=snr)\n",
    "    x = get_speech(x, _, 0.01, 0.01)\n",
    "    window_length = int(audio_params['window'] * audio_params['sr'])\n",
    "    hop_length = int(audio_params['hop'] * audio_params['sr'])\n",
    "    spec = librosa.stft(x, n_fft=audio_params['nfft'],\n",
    "                        hop_length=hop_length, \n",
    "                        win_length=window_length)\n",
    "    mag_spec = np.abs(spec)\n",
    "    mel_basis = librosa.filters.mel(audio_params['sr'], audio_params['nfft'],\n",
    "                                    n_mels=audio_params['nmels'])\n",
    "    mel_spec = np.dot(mel_basis, mag_spec)\n",
    "    mel_db = librosa.amplitude_to_db(mel_spec).T\n",
    "    last = len(mel_db) - tisv_frame - 1\n",
    "    beg = random.randint(0, last)\n",
    "    return mel_db[beg:beg+tisv_frame], x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = \"./dataset/dev/wav/\"\n",
    "test_root = \"./dataset/test/wav\"\n",
    "noise_root = \"./dataset/QUT-NOISE/split_noises/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = get_path_jsons(train_root)\n",
    "test_paths = get_path_jsons(test_root)\n",
    "noise_folders = [noise_root + file + \"/\" for file in os.listdir(noise_root)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speakers in train:  1211\n",
      "Total speakers in test:  40\n",
      "Total files in train:  148642\n",
      "Total files in test:  4874\n"
     ]
    }
   ],
   "source": [
    "print(\"Total speakers in train: \", len(train_paths))\n",
    "print(\"Total speakers in test: \", len(test_paths))\n",
    "print(\"Total files in train: \", sum([len(train_paths[root]) for root in train_paths]))\n",
    "print(\"Total files in test: \", sum([len(test_paths[root]) for root in test_paths]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxCelebDatasest(Dataset):\n",
    "    def __init__(self, train_paths, noise_folders, M, N, training=True):\n",
    "        self.batch_size = M\n",
    "        self.count = 0\n",
    "        self.segment_size = None\n",
    "        if training:\n",
    "            self.paths = train_paths\n",
    "            self.noise_folders = noise_folders\n",
    "            self.utterance_number = N\n",
    "        self.speakers = list(train_paths)\n",
    "        random.shuffle(self.speakers)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.speakers)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        speaker = self.speakers[idx]\n",
    "        wav_files = self.paths[speaker]\n",
    "        random.shuffle(wav_files)\n",
    "        wav_files = random.sample(wav_files, self.utterance_number)\n",
    "        mel_dbs = []\n",
    "        self.keep_segment_size()\n",
    "        for f in wav_files:\n",
    "            mel_db, _ = get_meldb(speaker + f, self.noise_folders, audio_params, self.segment_size)\n",
    "            mel_dbs.append(mel_db)\n",
    "        return torch.Tensor(mel_dbs)\n",
    "    \n",
    "    def keep_segment_size(self):\n",
    "        self.count += 1\n",
    "        if self.count % (self.batch_size + 1) == 0:\n",
    "            self.count = 1\n",
    "        if self.count == 1:\n",
    "            self.segment_size = random.randint(140, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GE2E(nn.Module):\n",
    "    def __init__(self, nlstm, dembed, dhid, dout, dropout=0.):\n",
    "        super(GE2E, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(dembed, dhid, num_layers=nlstm, batch_first=True)\n",
    "        self.linear = nn.Linear(dhid, dout)\n",
    "    def forward(self, inp):\n",
    "        lstm_out, _ = self.lstm(inp)\n",
    "        out = self.linear(lstm_out[:, -1, :])\n",
    "        out_norm = out / torch.norm(out, p=2, dim=0)\n",
    "        return out_norm\n",
    "            \n",
    "        \n",
    "class GE2ELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GE2ELoss, self).__init__()\n",
    "        self.w = nn.Parameter(torch.tensor(10.0).to(device), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.tensor(-5.0).to(device), requires_grad=True)\n",
    "        self.device = device\n",
    "    def forward(self, k, M):\n",
    "        groups = torch.split(k, M, dim=0)\n",
    "        centroids = torch.stack([torch.mean(group, dim=0) for group in groups])\n",
    "        sims = []\n",
    "        for idx, item in enumerate(k.repeat(N, 1, 1).transpose(1, 0)):\n",
    "            cur_centroid_idx = idx // M\n",
    "            for_stability = (centroids[cur_centroid_idx] - item[0]) * M / (M - 1)\n",
    "            updated_centroids = centroids.clone()\n",
    "            updated_centroids[cur_centroid_idx] = for_stability\n",
    "            sims.append(F.cosine_similarity(item, updated_centroids, dim=-1, eps=1e-8))\n",
    "        cossim = torch.stack(sims)\n",
    "        sim_matrix = self.w * cossim + self.b\n",
    "        loss = self.compute_loss(sim_matrix)\n",
    "        return loss\n",
    "    def compute_loss(self, sim_matrix):\n",
    "        loss = 0 \n",
    "        for idx, group in enumerate(torch.split(sim_matrix, M, dim=0)):\n",
    "            for utterance_sims in group:\n",
    "                cur = utterance_sims[idx]\n",
    "                rest = torch.cat([utterance_sims[:idx], utterance_sims[idx+1:]])\n",
    "                loss += - cur + torch.log(torch.sum(torch.exp(rest)))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_params = {'mean_snr': 15,\n",
    "               'sd_snr':10,\n",
    "               'hop': 0.01,\n",
    "               'window': 0.025,\n",
    "               'sr': 16000,\n",
    "               'tisv_frame': 180,\n",
    "               'nfft': 512,\n",
    "               'nmels': 40\n",
    "               }\n",
    "M = 4\n",
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VoxCelebDatasest(train_paths, noise_folders, M, N)\n",
    "train_loader = DataLoader(train_dataset, batch_size=M, shuffle=True, \n",
    "                          num_workers=1, drop_last=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, ge2e, ge2e_loss):\n",
    "    ge2e.train()\n",
    "    for batch_id, mel_db_batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        mel_db_batch = torch.reshape(mel_db_batch, (5*batch_size, mel_db_batch.size(2), mel_db_batch.size(3)))\n",
    "        embeddings = ge2e(mel_db_batch.cuda())\n",
    "        loss = ge2e_loss(embeddings, M)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(ge2e.parameters(), 3.0)\n",
    "        torch.nn.utils.clip_grad_norm_(ge2e_loss.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        print(batch_id, loss.item())\n",
    "        if batch_id == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge2e = GE2E(nlstm=3, dembed=40, dhid=768, dout=256).to(device)\n",
    "ge2e_loss = GE2ELoss().to(device)\n",
    "optimizer = torch.optim.SGD([\n",
    "                {'params': ge2e.parameters()},\n",
    "                {'params': ge2e_loss.parameters()}\n",
    "            ], lr= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 276.13323974609375\n",
      "1 275.7585144042969\n",
      "2 266.4223937988281\n",
      "3 262.79620361328125\n",
      "4 264.90338134765625\n",
      "5 251.80618286132812\n",
      "6 246.74217224121094\n",
      "7 244.52845764160156\n",
      "8 244.03240966796875\n",
      "9 239.78164672851562\n",
      "10 238.54244995117188\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, ge2e, ge2e_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing and noise generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise_paths = [\"./dataset/QUT-NOISE/\" + file for file in os.listdir(\"./dataset/QUT-NOISE/\") if file.endswith(\".wav\")]\n",
    "# for noise_path in tqdm(noise_paths):\n",
    "#     cur_noise_dir = noise_path.replace(\".wav\", \"\").replace(\"-1\", \"\").replace(\"-2\", \"\")\n",
    "#     cur_noise_dir_split = cur_noise_dir.split(\"/\")\n",
    "#     cur_noise_dir_split.insert(-1, \"split_noises\")\n",
    "#     cur_noise_dir = \"/\".join(cur_noise_dir_split) + \"/\"\n",
    "#     if not os.path.exists(cur_noise_dir):\n",
    "#         os.makedirs(cur_noise_dir)\n",
    "#     wpath = cur_noise_dir + noise_path.split(\"/\")[-1].replace(\".wav\", \"\")\n",
    "#     os.system(\"ffmpeg -i {} -f segment -segment_time 2 -c copy {}%05d.wav\".format(noise_path, wpath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment = random.randint(140, 180)\n",
    "# sample_speaker = random.sample(list(train_paths), 1)[0]\n",
    "# sample_file = random.sample(train_paths[sample_speaker], 1)[0]\n",
    "# path = sample_speaker + sample_file\n",
    "# meldb, mixed = get_meldb(path, noise_folders, audio_params, segment)\n",
    "# ipd.Audio(mixed, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
